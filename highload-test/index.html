<!doctype html>
<html lang="ru">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Сравнение открытых OLAP-систем Big Data</title>
</head>
<body>
    <article>
        <h1>Сравнение открытых OLAP-систем Big&nbsp;Data: ClickHouse, Druid и Pinot</h1>
        <p>Оригинал - <a href="https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7" target="_blank">https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7</a></p>

        <p><a href="https://clickhouse.yandex/" target="_blank">ClickHouse</a>, <a href="http://druid.io/" target="_blank">Druid</a> и <a href="https://github.com/linkedin/pinot" target="_blank">Pinot</a> – три открытых хранилища данных, которые&nbsp;позволяют выполнять аналитические запросы на&nbsp;больших объемах данных с&nbsp;интерактивными задержками. Эта&nbsp;статья - перевод <a href="https://medium.com/@leventov/comparison-of-the-open-source-olap-systems-for-big-data-clickhouse-druid-and-pinot-8e042a5ed1c7" target="_blank">подробного сравнения</a>, выполненного Романом Левентовым.</p>

        <h2>Источники информации</h2>
        <p>Подробности реализации <strong>ClickHouse</strong> стали мне известны от&nbsp;<a href="https://github.com/ztlpn" target="_blank">Алексея Зателепина</a>, одного&nbsp;из <strong>ключевых разработчиков проекта</strong>. Доступная на&nbsp;английском документация достаточно скудна – наилучшим источником информации служат последние четыре секции <a href="https://clickhouse.yandex/docs/en/development/architecture.html" target="_blank">данной страницы документации</a>.</p>

        <p><strong>Я сам участвую в&nbsp;развитии Druid</strong>, но у&nbsp;меня нет личной заинтересованности в&nbsp;этой системе - по&nbsp;правде говоря, скорее всего в&nbsp;ближайшее время я&nbsp;перестану заниматься её разработкой. Поэтому читатели могут рассчитывать на&nbsp;отсутствие какой-либо предвзятости.</p>

        <p>Всё, что я&nbsp;буду далее писать про&nbsp;<strong>Pinot</strong>, основывается на&nbsp;странице <a href="https://github.com/linkedin/pinot/wiki/Architecture" target="_blank">Архитектура в&nbsp;вики Pinot</a>, а&nbsp;также на&nbsp;других страницах вики в&nbsp;разделе “Проектная документация”. Последний раз они обновлялись в&nbsp;июне 2017&nbsp;года - больше, чем полгода назад.</p>

        <p>Рецензентами оригинальной статьи стали Алексей Зателепин и <a href="https://github.com/ludv1x" target="_blank">Виталий Людвиченко</a>(разработчики ClickHouse), <a href="https://github.com/gianm" target="_blank">Жан Мерлино</a> (самый активный разработчик Druid), <a href="https://github.com/kishoreg" target="_blank">Кишор Гопалакришна</a> (архитектор Pinot) и <a href="https://github.com/jfim" target="_blank">Жан-Француа Им</a> (разработчик Pinot). Мы присоединяемся к&nbsp;благодарности автора и полагаем, что это многократно повышает авторитетность статьи.</p> 

        <p><strong>Предупреждение</strong>: статья достаточно большая, поэтому вполне возможно вы&nbsp;захотите ограничиться прочтением раздела “Заключение” в&nbsp;конце.</p>

        <h2>Сходства между системами</h2>
        <h3>Связанные данные и вычисления</h3>
        <p><strong>На фундаментальном уровне, ClickHouse, Druid и Pinot похожи</strong>, поскольку они хранят данные и выполняют обработку запросов на&nbsp;одних&nbsp;и&nbsp;тех&nbsp;же узлах, уходя от&nbsp;“разъединенной” архитектуры BigQuery. Недавно я&nbsp;уже описывал несколько наследственных проблем со&nbsp;связанной архитектурой в&nbsp;случае Druid 
            [<a href="https://medium.com/@leventov/the-problems-with-druid-at-large-scale-and-high-load-part-1-714d475e84c9" target="_blank">1</a>, 
            <a href="https://medium.com/@leventov/the-challenges-of-running-druid-at-large-scale-and-future-directions-part-2-ef594ce298f2" target="_blank">2</a>]. 
            Открытого эквивалента для BigQuery на&nbsp;данный момент не&nbsp;существует (за&nbsp;исключением, разве&nbsp;что, <a href="http://drill.apache.org/" target="_blank">Drill</a>?) Возможным подходам к&nbsp;построению подобных открытых систем посвящена <a href="https://medium.com/@leventov/design-of-a-cost-efficient-time-series-store-for-big-data-88c5dc41af8e" target="_blank">другая статье в&nbsp;моем блоге</a>.</p>

        <h3>Отличия от&nbsp;Big&nbsp;Data SQL-систем: индексы и статическое распределение данных</h3>
        <p>Рассматриваемые в&nbsp;этой статье системы <strong>выполняют запросы быстрее</strong>, чем системы Big&nbsp;Data из&nbsp;семейства класса SQL-on-Hadoop: Hive, Impala, Presto и Spark, даже когда последние получают доступ к&nbsp;данным, хранящимся в&nbsp;колоночном формате - к&nbsp;примеру, Parquet или Kudu. Это происходит потому, что в&nbsp;ClickHouse, Druid и Pinot:</p>

        <ul>
            <li>
                Имеется <strong>свой собственный формат для хранения данных с&nbsp;индексами</strong>, и они тесно интегрированы с&nbsp;движками обработки запросов. Системы класса SQL-on-Hadoop обычно можно назвать агностиками относительно форматов данных и поэтому они менее “навязчивы” в&nbsp;бэкендах Big&nbsp;Data.
            </li>
            <li>
                <strong>Данные распределены относительно “статично”</strong> между узлами, и при распределенном выполнении запроса это можно использовать. Обратная сторона медали при&nbsp;этом в&nbsp;том, что ClickHouse, Druid и Pinot <strong>не&nbsp;поддерживают запросы, которые требуют перемещения большого количества данных</strong> между узлами - к&nbsp;примеру, join между двумя большими таблицами.
            </li>
        </ul>

        <h3>Отсутствие точечных обновлений и удалений</h3>
        <p>Находясь на&nbsp;другой стороне спектра баз данных, ClickHouse, Druid и Pinot <strong>не поддерживают точечные обновления и удаления</strong>, в противоположность колоночным системам вроде Kudu, InfluxDB и Vertica (?). Это даёт ClickHouse, Druid и Pinot возможность производить более эффективное колоночное сжатие и более агрессивные индексы, что означает <strong>большую эффективность использования ресурсов</strong> и быстрое выполнение запросов.</p>

        <p>Разработчики ClickHouse в&nbsp;Yandex планируют начать поддерживать <a href="https://clickhouse.yandex/docs/en/roadmap.html#q1-2018" target="_blank">обновления и удаления в&nbsp;будущем</a>, но я&nbsp;не&nbsp;уверен, будут ли это “настоящие” точечные запросы или обновления/удаления диапазонов данных.</p>

        <h3>Поглощение в&nbsp;стиле Big&nbsp;Data</h3>
        <p>Все три системы поддерживают потоковое поглощение данных из&nbsp;Kafka. Druid и Pinot поддерживают потоковую передачу данных стриминг в&nbsp;<a href="https://en.wikipedia.org/wiki/Lambda_architecture" target="_blank">Лямбда-стиле</a> и пакетное поглощение одних&nbsp;и&nbsp;тех&nbsp;же данных. ClickHouse поддерживает пакетные вставки напрямую, поэтому ему не&nbsp;требуется отдельная система пакетного поглощения подобная той, что используется в&nbsp;Druid и Pinot. Если вас интересуют подробности, то их вы сможете найти далее.</p>
        
        <h3>Проверено на&nbsp;крупном масштабе</h3>
        <p>Все три системы проверены на&nbsp;работоспособность в&nbsp;крупных масштабах: в&nbsp;<a href="https://yandex.com/blog/clickhouse/evolution-of-data-structures-in-yandex-metrica" target="_blank">Yandex.Metrica работает кластер ClickHouse</a>, состоящий из примерно десятка тысяч ядер CPU. В&nbsp;Metamarkets используется <a href="https://medium.com/@leventov/the-problems-with-druid-at-large-scale-and-high-load-part-1-714d475e84c9" target="_blank">кластер Druid аналогичного размера</a>. Один кластер Pinot в&nbsp;LinkedIn включает в&nbsp;себя “<a href="https://github.com/linkedin/pinot/issues/3#issuecomment-228497765" target="_blank">тысячи машин</a>”.</p>
        
        <h3>Незрелость</h3>
        <p>Все рассматриваемые в&nbsp;статье системы являются <strong>незрелыми по&nbsp;меркам открытых enterprise-систем Big&nbsp;Data</strong>. Однако, скорее всего они незрелы не более, чем среднестатистическая открытая система Big&nbsp;Data - но это совсем другая история. В ClickHouse, Druid и Pinot недостает некоторых очевидных оптимизаций и функциональности, и они кишат багами (насчет ClickHouse и Pinot я не уверен на все 100%, но не вижу причин, по которым они в этом плане были бы лучше Druid).</p>

        <p>Это плавно подводит нас к&nbsp;следующему важному разделу.</p>

        <h2>Про сравнение производительности и выбор системы</h2>
        <p>Я регулярно вижу в&nbsp;сети, как некоторые проводят сравнения систем больших данных: они берут набор своих данных, каким-либо образом “скармливают” его оцениваемой системе, а&nbsp;затем немедленно пытаются измерить производительность - сколько памяти или дискового пространства было занято, и насколько быстро выполнялись запросы. Причем понимание того, как устроены изнутри испытываемые ими системы, у&nbsp;них отсутствует. Затем, используя лишь подобные специфичные данные о&nbsp;производительности - иногда вместе со&nbsp;списками функциональности, которая им нужна и которая есть в&nbsp;системе <em>на&nbsp;настоящий момент</em>, - они в&nbsp;итоге делают свой выбор или, что еще хуже, выбирают написать свою собственную, “лучшую”&nbsp;систему с&nbsp;нуля.</p>

        <p>Такой подход мне кажется неправильным, по&nbsp;крайней&nbsp;мере он неприменим в&nbsp;отношении открытых OLAP-систем для&nbsp;Big&nbsp;Data. Задача создания системы Bid&nbsp;Data OLAP, которая смогла&nbsp;бы работать эффективно в&nbsp;большинстве сценариев использования и содержала&nbsp;бы все необходимые функции настолько велика, что я&nbsp;оцениваю ее реализацию как минимум в&nbsp;<strong>100 человеко-лет</strong>.</p>

        <p>На&nbsp;сегодня, ClickHouse, Druid и Pinot оптимизированы <em>только</em> для&nbsp;конкретных сценариев использования, которые требуются их разработчиком - и содержат по&nbsp;большей&nbsp;части лишь те&nbsp;функции, в&nbsp;которых нуждаются сами разработчики. Я могу гарантировать, что ваш&nbsp;случай обязательно “упрется” в&nbsp;те узкие места, с&nbsp;которыми разработчики рассматриваемых OLAP-систем еще не&nbsp;сталкивались - или же в&nbsp;те&nbsp;места, что их не&nbsp;интересуют.</p>

        <p>Не говоря уже о&nbsp;том, что упомянутый выше подход “забросить данные в&nbsp;систему, о&nbsp;которой вы ничего не&nbsp;знаете, и затем измерить её эффективность” весьма вероятно даст искаженный результат из-за серьезных “узких” мест, которые на&nbsp;самом&nbsp;деле могли&nbsp;бы быть исправлены <strong>простым изменением конфигурации</strong>, схемы данных или другим построением запроса.</p>

        <h3>CloudFlare: ClickHouse против Druid</h3>
        <p>Одним таким примером, хорошо иллюстрирующим описанную выше проблему, является пост Марека Вавруша о&nbsp;<a href="https://blog.cloudflare.com/how-cloudflare-analyzes-1m-dns-queries-per-second/#comment-3302778860" target="_blank">выборе между ClickHouse и Druid в&nbsp;Cloudflare</a>. Им потребовалось 4&nbsp;сервера ClickHouse (которые со&nbsp;временем превратились в&nbsp;9), и по&nbsp;их&nbsp;оценкам, для&nbsp;разворачивания аналогичной установки Druid им&nbsp;бы потребовались “сотни узлов”. Пусть Марек и признает, что <strong>сравнение является нечестным</strong>, поскольку Druid недостаёт “сортировки по&nbsp;первичному ключу”, он возможно даже не&nbsp;осознает, что достичь примерно того&nbsp;же самого эффекта в&nbsp;Druid возможно просто <a href="http://druid.io/docs/0.11.0/ingestion/index.html" target="_blank">установив правильный порядок измерений в&nbsp;“ingestion&nbsp;spec”</a> и произведя простую подготовку данных: обрезать значение колонки <code>__time</code> в&nbsp;Druid до&nbsp;некоей грубой детализации (к&nbsp;примеру, один час) и опционально добавить другую “длинно-типовую” колонку “precise_time”, если для&nbsp;некоторых запросов требуются более точные временные рамки. Да, это хак, но, как мы только что выяснили, и в&nbsp;Druid можно сортировать данные по&nbsp;какому-либо измерению перед <code>__time</code>, и это достаточно просто реализовать.</p>

        <p>Впрочем, я не&nbsp;стану спорить с&nbsp;их итоговым решением выбрать ClickHouse, поскольку на&nbsp;масштабе примерно в&nbsp;10 узлов и для&nbsp;их&nbsp;нужд ClickHouse мне тоже кажется лучшим выбором, чем Druid. Но&nbsp;сделанное ими заключение о&nbsp;том, что ClickHouse как минимум на&nbsp;порядок эффективнее (по&nbsp;меркам стоимости инфраструктуры), чем Druid - это серьезное заблуждение. На&nbsp;самом&nbsp;деле, из&nbsp;рассматриваемых нами сегодня систем, <strong>Druid предлагает наилучшую возможность для&nbsp;реально дешевых установок</strong> (смотрите раздел “Уровни узлов обработки запросов в&nbsp;Druid ” ниже).</p>

        <blockquote>
            <p>Когда вы выбираете систему OLAP Big&nbsp;Data, не&nbsp;сравнивайте то, насколько они сейчас хорошо подходят для&nbsp;вашего случая. Сейчас они все субоптимальны. Вместо этого, сравните, насколько быстро ваша компания способна заставить двигаться эти системы в&nbsp;том направлении, которое нужно именно вам.</p>
        </blockquote>

        <p>В&nbsp;силу своей фундаментальной архитектурной схожести, ClickHouse, Druid и Pinot имеют примерно один&nbsp;и&nbsp;тот&nbsp;же “предел” эффективности и оптимизации производительности. Здесь нет “волшебной таблетки”, которая позволила бы какой-либо из&nbsp;этих&nbsp;систем быть быстрее, чем остальные. Не&nbsp;позволяйте запутать себя тем&nbsp;фактом, что <em>в&nbsp;своем текущем состоянии</em> системы показывают себя очень по-разному в&nbsp;различных бенчмарках.</p>

        <p>Допустим, Druid не&nbsp;поддерживает “сортировку по&nbsp;первичному ключу” настолько хорошо, насколько это умеет ClickHouse - а&nbsp;ClickHouse в&nbsp;свою очередь не&nbsp;поддерживает “инвертированные индексы” столь же хорошо, как Druid, что дает данным системам преимущества с&nbsp;той или иной нагрузкой. <strong>Упущенные оптимизации могут быть реализованы в&nbsp;выбранной системе при помощи не&nbsp;таких&nbsp;уж и больших усилий</strong>, если у вас есть намерение и возможность решиться на&nbsp;подобный шаг.</p>
        <ul>
            <li>
                В&nbsp;вашей организации должны быть инженеры, способные прочитать, понять и модифицировать исходный код выбранной системы, к&nbsp;тому&nbsp;же у них должно быть на&nbsp;это время. Заметьте, что ClickHouse написан на&nbsp;C++, а Druid и Pinot —  на&nbsp;Java.
            </li>
            <li>
                Или же ваша организация должна подписать контракт с&nbsp;компанией, которая оказывает поддержку выбранной системы. Это будут <a href="https://www.altinity.com/" target="_blank">Altinity</a> для&nbsp;ClickHouse, <a href="https://imply.io/services" target="_blank">Imply</a> и <a href="https://hortonworks.com/open-source/druid/" target="_blank">Hortonworks</a> для&nbsp;Druid. Для&nbsp;Pinot таких компаний в&nbsp;данный момент нет.
            </li>
        </ul>

        <p>Другие сведения о&nbsp;разработке систем, которые вам стоит принять во&nbsp;внимание:</p>
        
        <ul>
            <li>
                Авторы ClickHouse, работающие в&nbsp;Yandex, утверждают, что они тратят 50% своего времени на&nbsp;создание функциональности, которая требуется им внутри компании, и другие 50% уходят на&nbsp;функции, который набирают большинство “голосов сообщества”. Однако, чтобы вы получили от&nbsp;этого факта преимущество, требуется, чтобы <strong>функции, которые нужны вам, были и наиболее востребованы сообществом</strong> ClickHouse.
            </li>
            <li>
                Разработчики Druid из&nbsp;Imply мотивированы работать над широко используемыми функциями, поскольку это позволит им максимально увеличить объем охвата своего бизнеса в&nbsp;будущем.
            </li>
            <li>
                Процесс разработки Druid сильно напоминает модель <a href="https://community.apache.org/apache-way/apache-project-maturity-model.html" target="_blank">Apache</a>, когда ПО несколько лет разрабатывается несколькими компаниями, у&nbsp;каждой из&nbsp;которых достаточно своеобразные и различные приоритеты, и среди них нет ведущей компании. ClickHouse и Pinot пока еще далеки от&nbsp;подобного этапа, поскольку ими занимаются соответственно лишь Yandex и Linkedin. Сторонний вклад в&nbsp;развитие Druid имеет минимальный шанс быть отклоненным в&nbsp;силу&nbsp;того, что он расходится с&nbsp;видением основного разработчика - ведь <strong>в&nbsp;Druid нет “основной” компании-разработчика</strong>.
            </li>
            <li>
                Druid поддерживает “API разработчика”, который позволяет привносить собственные типы колонок, механизмы агрегации, возможные варианты для&nbsp;«глубокого хранения» и&nbsp;пр., причем все это вы можете держать в&nbsp;кодовой базе, отдельной от&nbsp;самого ядра Druid. Данное API документировано разработчиками Druid, и они&nbsp;следят за&nbsp;его совместимостью с&nbsp;предыдущими версиями. Однако, оно&nbsp;недостаточно “взрослое”, и ломается практически с&nbsp;каждым новым релизом Druid. Насколько мне известно, в&nbsp;ClickHouse и Pinot схожие API не&nbsp;поддерживаются.
            </li>
            <li>
                Согласно Github, <strong>над&nbsp;Pinot работает наибольшее число людей</strong> - по&nbsp;всей видимости, лишь за&nbsp;прошлый год в&nbsp;Pinot было вложено <a href="https://github.com/linkedin/pinot/graphs/contributors?from=2017-01-24&to=2018-01-24&type=c" target="_blank">не&nbsp;менее 10&nbsp;человеко-лет</a>. Для ClickHouse эта цифра составляет примерно 6&nbsp;человеко-лет, а для Druid - 7. В&nbsp;теории, это должно означать, что Pinot улучшается быстрее всех остальных систем, которые мы рассматриваем.
            </li>
        </ul>

        <p>Архитектуры Druid и Pinot почти что идентичны друг другу, в&nbsp;то время как ClickHouse стоит слегка в&nbsp;стороне. Поэтому сначала мы сравним ClickHouse c&nbsp;“обобщенной” архитектурой Druid/Pinot, а&nbsp;затем обсудим мелкие различия между Druid и Pinot.</p>

        <h2>Различия между ClickHouse и Druid/Pinot</h2>
        <h3>Управление данными: Druid и Pinot</h3>

        <p>В Druid и Pinot, все&nbsp;данные в&nbsp;каждой “таблице” (как бы она не&nbsp;называлась в&nbsp;терминологии этих систем) разбиваются на&nbsp;указанное количество частей. По&nbsp;временой оси, данные обычно разделены с&nbsp;заданным интервалом. Затем эти части данных «запечатываются» индивидуально в&nbsp;самостоятельные автономные сущности, называемые «сегментами». Каждый&nbsp;сегмент включает в&nbsp;себя метаданные таблицы, сжатые столбчатые данные и индексы.</p>

        <p>Сегменты хранятся в&nbsp;файловой системе хранилища «глубокого хранения» (например,&nbsp;HDFS) и могут быть загружены на&nbsp;узлы обработки запросов, но&nbsp;последние не&nbsp;отвечают за&nbsp;устойчивость сегментов, поэтому узлы обработки запросов могут быть заменены относительно свободно. <strong>Сегменты не&nbsp;привязаны жестко к&nbsp;конкретным узлам</strong> и могут быть загружены на&nbsp;те или другие узлы. Специальный выделенный сервер (который называется “координатором” в&nbsp;Druid и “контроллером” в&nbsp;Pinot, но я ниже обращаюсь к&nbsp;нему как&nbsp;к&nbsp;“мастеру”) отвечает за&nbsp;присвоение сегментов узлам, и перемещению сегментов между узлами, если потребуется.</p>

        <p>Это не&nbsp;противоречит тому, что я отмечал выше, все&nbsp;три системы имеют статическое распределение данных между узлами, поскольку загрузки сегментов и их&nbsp;перемещения в Druid - и насколько я понимаю в&nbsp;Pinot - являются дорогими операциями и потому не выполняются для каждой отдельной очереди, а&nbsp;происходят обычно раз в&nbsp;несколько минут/часов/дней.</p>

        <p>Метаданные сегментов хранятся в&nbsp;ZooKeeper - напрямую в&nbsp;случае Druid, и при помощи фреймворка <a href="http://helix.apache.org/" target="_blank">Helix</a> в&nbsp;Pinot. В Druid метаданные также хранятся в&nbsp;базе SQL, об&nbsp;этом будет подробнее в&nbsp;разделе “Различия между Druid и Pinot”.</p>


        <h3>Управление данными: ClickHouse</h3>
        <p>В&nbsp;ClickHouse нет “сегментов”, содержащих данные, попадающие в&nbsp;конкретные временные диапазоны. В&nbsp;нем нет “глубокого хранения” для&nbsp;данных, узлы в&nbsp;кластере ClickHouse также отвечают и за&nbsp;обработку запросов, и за&nbsp;постоянство/устойчивость данных, хранящихся на&nbsp;них. Так что вам <strong>не&nbsp;потребуется HDFS</strong> или облачное хранилище данных вроде Amazon S3.</p>

        <p>В&nbsp;ClickHouse имеются секционированные таблицы, состоящие из&nbsp;указанного набора узлов. Здесь нет “центральной власти” или сервера метаданных. Все узлы, между которыми разделена та или&nbsp;иная таблица, содержат полные, идентичные копии метаданных, включая адреса всех остальных узлов, на&nbsp;которых хранятся секции этой таблицы.</p>

        <p>Метаданные секционированной таблицы включают “весы” узлов для распределения свежезаписываемых данных - к&nbsp;примеру, 40% данных должны идти на&nbsp;узел A, 30% на&nbsp;узел B и 30% на&nbsp;C. Обычно же распределение должно происходить равномерно, “перекос”, как в&nbsp;этом примере, требуется только тогда, когда к&nbsp;секционированной таблице добавляется новый узел и нужно побыстрее заполнить его какими-либо данными. <strong>Обновления этих “весов” должны выполняться вручную</strong> администраторами кластера ClickHouse, или&nbsp;же автоматизированной системой, построенной поверх ClickHouse.</p>

        <h3>Управление данными: сравнение</h3>
        <p>Подход к управлению данными в&nbsp;ClickHouse проще, чем в&nbsp;Druid и Pinot: не&nbsp;требуется “глубокого хранилища”, всего один тип узлов, не&nbsp;требуется выделенного сервера для управления данными. Но&nbsp;подход ClickHouse приводит к&nbsp;некоторым трудностям, когда любая таблица данных вырастает настолько большой, что требуется ее разбиение между десятком или более узлов: коэффициент усиления запроса становится настолько&nbsp;же велик, насколько и фактор секционирования - даже для запросов, которые покрывают небольшой интервал данных:</p>

        <figure>
            <div>
                <img src="img/pic-1.png" alt="" width="626" height="390">
            </div>
            <figcaption>Компромисс распределения данных в ClickHouse</figcaption>
        </figure>

        <p>В&nbsp;примере, показанном на&nbsp;изображении выше, данные таблицы распределены между тремя узлами в&nbsp;Druid/Pinot, но&nbsp;запрос по&nbsp;малому интервалу данных обычно затрагивает лишь два из&nbsp;них (до&nbsp;той&nbsp;поры, пока интервал не&nbsp;пересечет пограничный интервал сегмента). В&nbsp;ClickHouse, любые запросы будут вынуждены затронуть три узла – если таблица сегментирована между тремя узлами. В&nbsp;данном примере разница не&nbsp;выглядит настолько существенно, однако представьте себе, что&nbsp;случится, если число узлов достигнет&nbsp;100 – в&nbsp;то&nbsp;время как фактор сегментирования по-прежнему может быть равен, например, 10 в&nbsp;Druid/Pinot.</p>

        <p>Чтобы смягчить эту проблему, самый большой кластер ClickHouse в&nbsp;Яндексе, состоящий из&nbsp;сотен узлов, в&nbsp;действительности разбит на&nbsp;многие «под-кластеры» с&nbsp;несколькими десятками узлов в&nbsp;каждом. Кластер ClickHouse используется в&nbsp;работе с&nbsp;аналитикой по&nbsp;веб-сайтам, и каждая точка данных имеет измерение «ID вебсайта». Существует жесткая привязка каждого ID сайта к&nbsp;конкретному под-кластеру, куда идут все данные для&nbsp;этого идентификатора сайта. Поверх кластера ClickHouse есть слой бизнес-логики, который управляет этим разделением данных при&nbsp;поглощении данных и выполнении запросов. К&nbsp;счастью, в&nbsp;их сценариях использования совсем немного запросов затрагивают несколько идентификаторов сайтов, и подобные запросы идут не&nbsp;от&nbsp;пользователей сервиса, поэтому у&nbsp;них нет жесткой привязки к&nbsp;реальному времени согласно соглашению об&nbsp;уровне услуг.</p>

        <p>Другим недостатком подхода ClickHouse является то, что, когда кластер растет очень быстро, данные не&nbsp;могут перебалансироваться автоматически без участия человека, который вручную поменяет «веса» узлов в&nbsp;разбиваемой таблице.</p>

        <h3>Уровни узлов обработки запросов в&nbsp;Druid</h3>
        <p>Управление данными при&nbsp;помощи сегментов «проще себе представить» - эта концепция хорошо ложится на&nbsp;наши когнитивные способности. Сами сегменты можно перемещать между&nbsp;узлами относительно просто. Эте&nbsp;две причины позволили Druid реализовать “<em>разделение на&nbsp;уровни</em>” узлов, занимающихся обработкой запросов: старые данные автоматически перемещаются на&nbsp;сервера с&nbsp;относительно большими дисками, но&nbsp;меньшим количеством памяти и CPU, что&nbsp;позволяет <strong>значительно снизить стоимость большого рабочего кластера Druid</strong> за&nbsp;счет замедления запросов к&nbsp;более старым данным.</p>

        <p>Эта&nbsp;функция позволяет Metamarkets экономить сотни тысяч долларов расходов на&nbsp;инфраструктуру Druid каждый месяц - в&nbsp;противовес тому варианту, если&nbsp;бы использовался “плоский” кластер.</p>

        <figure>
            <div>
                <img src="img/pic-2.png" alt="" width="344" height="336">
            </div>
            <figcaption>Уровни узлов обработки запросов в&nbsp;Druid</figcaption>
        </figure>

        <p>Насколько мне известно, в&nbsp;ClickHouse и Pinot пока еще нет похожей функциональности - предполагается, что все узлы в&nbsp;их кластерах одинаковы.</p>

        <p>В&nbsp;силу того, что архитектура Pinot весьма схожа с&nbsp;архитектурой Druid, как мне кажется, будет не&nbsp;слишком сложно добавить аналогичную функцию в&nbsp;Pinot. Тяжелее будет в&nbsp;случае с&nbsp;ClickHouse, поскольку для&nbsp;реализации данной функции крайне полезно использование концепта “сегментов”, однако это всё равно возможно.</p>

        <h3>Репликация данных: Druid и Pinot</h3>
        <p>Единицей репликации в&nbsp;Druid и Pinot является единичный сегмент. Сегменты реплицируются на&nbsp;уровне «глубокого хранения» (например, в&nbsp;три реплики на&nbsp;HDFS, или при помощи хранилища BLOB-объектов в&nbsp;Amazon S3), и на&nbsp;уровне обработки запросов: обычно и в&nbsp;Druid и в&nbsp;Pinot, каждый сегмент загружается на&nbsp;два различных узла. «Мастер»-сервер мониторит уровни репликации для каждого сегмента и загружает сегмент на&nbsp;какой-либо сервер, если фактор репликации падает ниже заданного уровня (например, если какой-либо из&nbsp;узлов перестаёт отвечать).</p>

        <h3>Репликация данных: ClickHouse</h3>
        <p>Единицей репликации в&nbsp;ClickHouse является секция таблицы на&nbsp;сервере (например, все данные из&nbsp;какой-либо таблицы, хранящиеся на&nbsp;сервере). Аналогично секционированию, репликация в&nbsp;ClickHouse является скорее «статической и конкретной», чем «в&nbsp;облачном стиле»: несколько серверов знают, что они являются репликами друг друга (для&nbsp;некоторой конкретной таблицы; в&nbsp;случае другой таблицы, конфигурация репликации может отличаться). Репликация предоставляет и устойчивость, и доступность запросов. Когда повреждается диск на&nbsp;одном узле, данные не&nbsp;теряются, поскольку они хранятся еще и на&nbsp;другом узле. Когда какой-либо узел временно не&nbsp;доступен, запросы могут быть перенаправлены на&nbsp;реплику.</p>

        <p>В&nbsp;самом большом кластере ClickHouse в&nbsp;Яндексе есть два одинаковых набора узлов в&nbsp;различных дата-центрах, и они спарены. В&nbsp;каждой паре узлы являются репликами друг&nbsp;друга (используется фактор репликации, равный двум), и они расположены в&nbsp;различных дата-центрах.</p>

        <p>ClickHouse полагается на&nbsp;ZooKeeper для&nbsp;управления репликацией – поэтому, если вам не&nbsp;нужна репликация, то вам не&nbsp;нужен и ZooKeeper. Это означает, что ZooKeeper не&nbsp;потребуется и для&nbsp;ClickHouse, развернутого на&nbsp;одиночном узле.</p>

        <h3>Поглощение данных: Druid и Pinot</h3>
        <p>В Druid и Pinot узлы обработки запросов специализируются на&nbsp;загрузке сегментов и обслуживают запросы к&nbsp;данным в&nbsp;сегментах; они не&nbsp;занимаются накоплением новых данных и производством новых сегментов.</p>

        <p>Когда таблица может обновляться с&nbsp;задержкой в&nbsp;час или более, сегменты создаются при&nbsp;помощи движков пакетной обработки – к&nbsp;примеру, Hadoop или Spark. И в&nbsp;Druid, и в&nbsp;Pinot есть первоклассная поддержка Hadoop из&nbsp;коробки. Существует <a href="https://github.com/metamx/druid-spark-batch" target="_blank">сторонний плагин для&nbsp;поддержки индексации Druid в&nbsp;Spark</a>, но в&nbsp;данный момент официально он не&nbsp;поддерживается. Насколько мне известно, в&nbsp;Pinot такого уровня поддержки Spark пока нет, то&nbsp;есть вы должны&nbsp;быть готовы разобраться с&nbsp;интерфейсами Pinot и кодом, а&nbsp;затем самостоятельно написать код на&nbsp;Java/Scala, пусть это и не&nbsp;должно быть слишком сложно. (Впрочем, с&nbsp;момента публикации оригинальной статьи поддержка Spark в&nbsp;Pinot <a href="https://github.com/linkedin/pinot/pull/2388" target="_blank">была внесена контрибьютором</a>).</p>

        <p>Когда таблица должна обновляться в&nbsp;реальном времени, здесь приходит на&nbsp;помощь идея “реалтаймовых» узлов, которые делают три вещи: принимает новые данные из&nbsp;Kafka (Druid поддерживает и другие источники), обслуживает запросы с&nbsp;недавними данными, создает сегменты в&nbsp;фоне и затем записывает их в&nbsp;“глубокое хранилище”.</p>

        <h3>Поглощение данных: ClickHouse</h3>
        <p>Тот факт, что ClickHouse не&nbsp;требуется готовить “сегменты”, содержащие все данные и попадающие в&nbsp;заданные временные интервалы, позволяет строить более простую архитектуру поглощения данных. ClickHouse не&nbsp;требуется ни&nbsp;пакетный движок обработки вроде Hadoop, ни&nbsp;“реалтаймовые” узлы. Обычные узлы ClickHouse - те&nbsp;же самые, что занимаются хранением данных и обслуживают запросы к&nbsp;ним - напрямую принимают пакетные записи данных.</p>

        <p>Если таблица разбита на&nbsp;сегменты, то&nbsp;узел, который принимает пакетную запись (например, 10к&nbsp;строк) распределяет данные согласно “весам” (смотрите раздел ниже). Строки записываются одним пакетом, который формирует небольшое “множество”. Множество немедленно конвертируется в&nbsp;колоночный формат. На&nbsp;каждом узле ClickHouse работает фоновый процесс, который объединяет наборы строк в&nbsp;еще большие наборы. Документация ClickHouse сильно завязана на&nbsp;принцип, известный как “MergeTree”, и подчеркивает схожесть его работы с&nbsp;<a href="https://ru.wikipedia.org/wiki/LSM-%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE" target="_blank">LSM-деревом</a>, хотя меня это слегка смущает, поскольку данные не&nbsp;организованы в&nbsp;деревья - они лежат в&nbsp;плоском колончатом формате.</p>

        <h3>Поглощение данных: сравнение</h3>
        <p>Поглощение данных в&nbsp;Druid и Pinot является “тяжелым”: оно состоит из&nbsp;нескольких различных сервисов, и управление ими - это&nbsp;тяжелый труд.</p>

        <p>Поглощение данных в&nbsp;ClickHouse гораздо проще (что компенсируется сложностью управления “историческими” данными - т.е. данными не&nbsp;в&nbsp;реальном времени), но и здесь есть один момент: вы должны иметь возможность собирать данные в&nbsp;пакеты до&nbsp;самого ClickHouse. Автоматическое поглощение и пакетный сбор данных из&nbsp;<a href="https://clickhouse.yandex/docs/en/table_engines/kafka.html" target="_blank">Kafka доступно “из&nbsp;коробки”</a>, но если у&nbsp;вас используется другой источник данных в&nbsp;реальном времени (здесь подразумевается всё, что угодно, в&nbsp;диапазоне между инфраструктурой запросов, альтернативной Kafka, и стриминговых движков обработки, вплоть до&nbsp;различных HTTP-endpoint), то вам придется создать промежуточный сервис по&nbsp;сбору пакетов, или&nbsp;же внести код напрямую в&nbsp;ClickHouse.</p>

        <h3>Выполнение запроса</h3>
        <p>В&nbsp;<strong>Druid и Pinot</strong> имеется отдельный слой узлов, называемых “<em>брокерами</em>”, которые принимают все запросы к&nbsp;системе. Они определяют, к&nbsp;каким “историческим” (<em>содержащим данные не&nbsp;в&nbsp;реальном времени</em>) узлам обработки запросов должны быть отправлены подзапросы, основываясь на&nbsp;отображении сегментов в&nbsp;узлы, в&nbsp;которых сегменты загружаются. Брокеры хранят информацию об&nbsp;отображении в&nbsp;памяти. Брокер-узлы отправляют дальше подзапросы к&nbsp;узлам обработки запросов, и когда результаты этих подзапросов возвращаются, брокер объединяет их и возвращает финальный комбинированный результат пользователю.</p>

        <p>Я не&nbsp;берусь предполагать, зачем при&nbsp;проектировании Druid и Pinot было принято решение о&nbsp;введении еще одного типа узлов. Однако, теперь они кажутся их неотъемлемой частью, поскольку, когда общее количество сегментов в&nbsp;кластере начинает превышать десять миллионов, информация об&nbsp;отображении сегментов в&nbsp;узлы начинает занимать гигабайты памяти. Это очень расточительно – выделять столько много памяти на&nbsp;каждом узле для&nbsp;обработки запросов. Вот вам и еще&nbsp;один недостаток, который накладывается на&nbsp;Druid и Pinot их&nbsp;«сегментированной» архитектурой управления данными.</p>

        <p>В&nbsp;<strong>ClickHouse</strong> выделять отдельный набор узлов под&nbsp;“брокер запросов” обычно не&nbsp;требуется. Существует специальный, эфемерный <a href="https://clickhouse.yandex/docs/en/table_engines/distributed.html" target="_blank">“распределенный” тип таблицы</a> в&nbsp;ClickHouse, который может&nbsp;быть установлен на&nbsp;любом узле, и запросы к&nbsp;этой таблице будут делать все то&nbsp;же, за что отвечают брокер-узлы в&nbsp;Druid и Pinot. Обычно подобные эфемерные таблицы размещаются на&nbsp;каждом узле, который участвует в&nbsp;секционированной таблице, так что на&nbsp;практике каждый узел может быть “входной точкой” для&nbsp;запроса в&nbsp;кластер ClickHouse. Этот узел может выпускать необходимые подзапросы к&nbsp;другим секциями, обрабатывать свою часть запроса самостоятельно и затем объединять её с&nbsp;частичными результатами от&nbsp;других секций.</p>

        <p>Когда узел (или один из&nbsp;процессинговых узлов в&nbsp;ClickHouse, или брокер-узел в&nbsp;Druid и Pinot) выпускает подзапросы к&nbsp;другим, и один или несколько подзапросов по&nbsp;какой-либо причине заканчиваются неудачей, ClickHouse и Pinot обрабатывают эту ситуацию правильно: они объединяют результаты успешно выполненных подзапросов вместе, и всё&nbsp;равно возвращают частичный результат пользователю. <a href="https://medium.com/@leventov/the-problems-with-druid-at-large-scale-and-high-load-part-1-714d475e84c9" target="_blank">Druid этой функции сейчас очень недостает</a>: если в&nbsp;нем выполнение подзапроса заканчивается неудачей, то неудачей закончится и весь запрос целиком.</p>

        <h3>ClickHouse vs. Druid или Pinot: Выводы</h3>
        “Сегментированный” подход к&nbsp;управлению данными в&nbsp;Druid и Pinot против более простого управления данными в&nbsp;ClickHouse определяет многие аспекты систем. Однако, важно заметить, что это различие оказывает небольшое (или не&nbsp;оказывает вовсе) влияние на&nbsp;потенциальную эффективность сжатия (впрочем, история про&nbsp;компрессию для&nbsp;всех трех систем имеет печальный конец по&nbsp;нынешнему состоянию дел), или на&nbsp;скорость обработки запросов.

        ClickHouse похож на&nbsp;традиционные RDMBS, например, PostgreSQL. В&nbsp;частности, ClickHouse можно развернуть на&nbsp;всего один сервер. Если планируемый размер невелик - скажем, не&nbsp;больше порядка 100 ядер CPU для&nbsp;обработки запросов и 1&nbsp;TB данных, я&nbsp;бы сказал, что ClickHouse имеет значительные преимущества перед&nbsp;Druid и Pinot в&nbsp;силу своей простоты и отсутствия необходимости в&nbsp;дополнительных типах узлов, таких как “мастер”, “узлы поглощения в&nbsp;реальном времени”, “брокеры”. На&nbsp;этом поле, ClickHouse соревнуется скорее с&nbsp;InfluxDB, чем с&nbsp;Druid или Pinot.

        Druid and Pinot похож на&nbsp;системы Big Data вроде HBase. Здесь в&nbsp;виду имеются не&nbsp;характеристики производительности, а&nbsp;зависимость от&nbsp;ZooKeper, зависимость от&nbsp;персистентного реплицируемого хранилища (к&nbsp;примеру, HDFS), сосредоточение внимания на&nbsp;устойчивости к&nbsp;отказам отдельных узлов, а&nbsp;также автономная работа и управление данными, не&nbsp;требующими постоянного внимания человека.

        Для широкого спектра приложений, ни&nbsp;ClickHouse, ни&nbsp;Druid или Pinot не&nbsp;являются очевидными победителями. В&nbsp;первую очередь, вы должны принимать во&nbsp;внимание вашу способность разобраться с&nbsp;исходным кодом системы, исправлять баги, добавлять новые функции и т.д. Это подробнее обсуждается в&nbsp;разделе “Про сравнение производительности и выбор системы”.

        Во-вторых, вам стоит взглянуть на&nbsp;таблицу ниже. Каждая ячейка в&nbsp;этой таблице описывает свойство приложения, которое позволит определить предпочтительную систему. Строки отсортированы не&nbsp;в&nbsp;порядке важности. Важность различных свойств может разниться от&nbsp;приложения к&nbsp;приложению, но в&nbsp;целом можно применить следующий подход: если ваше приложение соответствует подавляющему большинству строк со&nbsp;свойствами в&nbsp;одной из&nbsp;колонок, то относящаяся к&nbsp;ней система в&nbsp;вашем случае является предпочтительным выбором.

        <table>
            <tr>
                <th>ClickHouse</th>
                <th>Druid или Pinot</th>
            </tr>
            <tr>
                <td>В организации есть эксперты по C++</td>
                <td>В организации есть эксперты по Java</td>
            </tr>
            <tr>
                <td>Малый кластер</td>
                <td>Большой кластер</td>
            </tr>
            <tr>
                <td>Немного таблиц</td>
                <td>Много таблиц</td>
            </tr>
            <tr>
                <td>Один набор данных</td>
                <td>Несколько несвязанных наборов данных</td>
            </tr>
            <tr>
                <td>Таблицы и данные находятся в&nbsp;кластере перманентно</td>
                <td>Таблицы и наборы данных периодически появляются в&nbsp;кластере и удаляются из&nbsp;него</td>
            </tr>
            <tr>
                <td>Размер таблиц (и интенсивность запросов к&nbsp;ним) остается стабильным во&nbsp;времени</td>
                <td>Таблицы значительно растут и сжимаются</td>
            </tr>
            <tr>
                <td>Однородные запросы (их тип, размер, распределение по&nbsp;времени суток и&nbsp;т.д.)</td>
                <td>Разнородные запросы</td>
            </tr>
            <tr>
                <td>В&nbsp;данных есть измерение, по&nbsp;которому оно может&nbsp;быть сегментировано, и почти не&nbsp;выполняется запросов, которые затрагивают данные, расположенные в&nbsp;нескольких сегментах</td>
                <td>Подобного измерения нет, и запросы часто затрагивают данные, расположенные во&nbsp;всем кластере</td>
            </tr>
            <tr>
                <td>Облако не&nbsp;используется, кластер должен&nbsp;быть развернут на&nbsp;специфическую конфигурацию физических серверов</td>
                <td>Кластер развернут в&nbsp;облаке</td>
            </tr>
            <tr>
                <td>Нет существующих кластеров Hadoop или Spark</td>
                <td>Кластеры Hadoop или Spark уже существуют и могут&nbsp;быть использованы</td>
            </tr>
        </table>

        <p><strong>Примечание</strong>: ни&nbsp;одно из&nbsp;свойств выше не&nbsp;означает, что вы должны использовать соответствующую систему (системы), или избегать другую. К&nbsp;примеру, если планируется, что ваш кластер будет большим, это не&nbsp;значит, что вы обязательно должны рассматривать только Druid или Pinot, исключив ClickHouse. Скорее всего, в&nbsp;данной ситуации Druid или Pinot могут&nbsp;быть лучшим выбором, но другие полезные свойства могут перевесить чашу весов в&nbsp;сторону ClickHouse, который для&nbsp;некоторых приложений является оптимальным выбором даже для&nbsp;больших кластеров.</p>

        <h2>Различия между Druid и Pinot</h2>
        <p>Как уже не&nbsp;раз отмечалось в&nbsp;данной статье, Druid и Pinot имеют весьма похожие архитектуры. Есть несколько достаточно заметных особенностей, которые есть в&nbsp;одной системе и отсутствуют в&nbsp;другой, и областей, в&nbsp;которых каждая из&nbsp;систем развита гораздо сильнее другой. Тем&nbsp;не&nbsp;менее, всё, о&nbsp;чем я собираюсь упомянуть ниже, можно воспроизвести в&nbsp;другой системе, приложив разумное количество усилий.</p>

        <p>Между&nbsp;Druid и Pinot существует лишь <strong>одно существенное различие</strong>, которое слишком велико для&nbsp;того, чтобы от&nbsp;него избавились в&nbsp;обозримом будущем - это <strong>реализация управления сегментами</strong> в&nbsp;мастер-ноде. Кстати, разработчики обеих систем наверняка не&nbsp;хотели&nbsp;бы этого делать в&nbsp;любом случае, поскольку оба подхода имеют свои “за” и “против” - среди&nbsp;них нет такого, который был&nbsp;бы лучше.</p>

        <h3>Управление сегментами в&nbsp;Druid</h3>
        <p>Мастер-нода в&nbsp;Druid (и ни&nbsp;один из&nbsp;узлов в&nbsp;Pinot) не&nbsp;отвечают за&nbsp;сохранность метаданных в&nbsp;сегментах данных в&nbsp;кластере, и текущее отображение между сегментами и узлами обработки данных, на&nbsp;которых загружены сегменты. Эта информация хранится в&nbsp;ZooKeeper. Однако, Druid в&nbsp;дополнение хранит эту информацию еще и в&nbsp;SQL базе данных, которая необходима для&nbsp;развертывания кластера Druid. Не&nbsp;могу сказать, с&nbsp;какой целью было принято такое решение, но сейчас оно дает следующие преимущества:</p>

        <ul>            
            <li>
                В&nbsp;ZooKeeper <strong>хранится меньше данных</strong>. Только минимум информации об&nbsp;отображении идентификатора сегмента на&nbsp;список узлов, занимающихся обработкой запросов, куда загружен сегмент, сохраняется в&nbsp;ZooKeeper. Оставшиеся метаданные, к&nbsp;примеру, размер сегмента, список измерений и метрики, и&nbsp;т.д. - хранятся только в&nbsp;SQL базе данных.
            </li>

            <li>
                Когда сегменты данных вытесняются из&nbsp;кластера, поскольку они становятся слишком старыми (это общая функция всех баз данных временных рядов - она есть и в&nbsp;ClickHouse, и в&nbsp;Druid, и в&nbsp;Pinot), они выгружаются из&nbsp;узлов обработки запросов и их метаданные удаляются из&nbsp;ZooKeeper, но не&nbsp;из&nbsp;“глубокого хранилища” и не&nbsp;из&nbsp;базы данных SQL. Пока они не&nbsp;будут удалены из&nbsp;этих мест вручную, <strong>остается возможность “оживить” действительно старые данные быстро</strong>, если он потребуются для&nbsp;построения отчетов или исследований.
            </li>

            <li>
                Вряд ли это планировалось с&nbsp;самого начала, но теперь есть планы сделать <strong>зависимость Druid от&nbsp;ZooKeeper опциональной</strong>. Сейчас ZooKeeper используется для&nbsp;трех различных функций: управления сегментами, обнаружения сервисов и хранения свойств (например, для&nbsp;управления поглощением данных в&nbsp;реальном времени). Обнаружение сервисов может <a href="https://groups.google.com/d/msg/druid-development/eIWDPfhpM_U/Em06lGjhAwAJ" target="_blank">быть предоставлено Consul</a>. Управление сегментами может&nbsp;быть реализовано <a href="https://groups.google.com/d/msg/druid-development/tWnwPyL0Vk4/2uLwqgQiAAAJ" target="_blank">при&nbsp;помощи HTTP-команд</a>, и оно доступно нам благодаря тому, что функции хранения в&nbsp;ZooKeeper “бекапится” в&nbsp;базе SQL.
            </li>
        </ul>

        <p>То, что нам приходится иметь в&nbsp;зависимостях базу данных SQL, приводит к&nbsp;большей нагрузке на&nbsp;эксплуатацию, особенно, если в&nbsp;компании еще не&nbsp;использовалась какая-либо БД&nbsp;SQL. Druid поддерживает MySQL и PostgreSQL, есть и расширение для&nbsp;Microsoft SQL Server. Кроме&nbsp;того, когда Druid рразворачивается в&nbsp;облаке, можно использовать стандартные сервисы для&nbsp;управления RDBMS - к&nbsp;примеру, Amazon&nbsp;RDS.</p>

        <h3>Управление сегментами в&nbsp;Pinot</h3>
        <p>В&nbsp;отличие от&nbsp;Druid, который реализует всю логику управления сегментами самостоятельно и полагается только на&nbsp;<a href="https://curator.apache.org/" target="_blank">Curator</a> для&nbsp;взаимодействия с&nbsp;ZooKeeper, Pinot делегирует большую часть логики управления сегментами и кластерами на&nbsp;<a href="https://helix.apache.org/" target="_blank">фреймворк Helix</a>.</p>

        <p>С&nbsp;одной стороны, я могу понять, что это дает разработчикам Pinot возможность сосредоточиться на&nbsp;других частях их системы. В&nbsp;Helix возможно меньше багов, чем в&nbsp;логике внутри самого Druid, поскольку он тестируется в&nbsp;других условиях и поскольку в&nbsp;него, предположительно, было вложено гораздо больше рабочего времени.</p>

        <p>С&nbsp;другой стороны, Helix возможно ограничивает Pinot своими “рамками фреймворка”. Helix, и, следовательно, <strong>Pinot, скорее всего будут зависеть от&nbsp;ZooKeeper всегда</strong>.</p>

        <p>Далее я собираюсь перечислить менее важные различия между&nbsp;Druid и Pinot - в&nbsp;том смысле, что если у&nbsp;вас возникнет серьезное желание повторить одну из&nbsp;этих функций в&nbsp;вашей системе, то это будет вполне осуществимо.</p>

        <h3>«Проталкивание предикатов» в&nbsp;Pinot</h3>
        <p>Если во&nbsp;время поглощения данные секционируются в&nbsp;Kafka по&nbsp;каким-либо ключам измерений, Pinot создает сегменты, которые содержат информацию об&nbsp;этом разбиении и затем, когда выполняется запрос с&nbsp;предикатом на&nbsp;данном измерении, брокер-узел фильтрует сегменты таким образом, чтобы как можно меньше сегментов и узлов обработки запросов было затронуто.</p>

        <p>Эта концепция в&nbsp;оригинале называется “<strong>predicate pushdown</strong>” и важна для поддержания высокой производительности в&nbsp;некоторых приложениях.</p>

        <p>На&nbsp;данный момент Druid поддерживает разбиение по&nbsp;ключам, если сегменты были созданы в&nbsp;Hadoop, но еще не&nbsp;поддерживает сегменты, созданные во&nbsp;время поглощения в&nbsp;реальном времени. Druid сейчас не&nbsp;реализует функцию «проталкивания предикатов» на&nbsp;брокеры.</p>

        <h3>“Сменный” Druid и своевольный Pinot</h3>
        <p>Поскольку Druid используют различные организации и в&nbsp;его разработке принимают участие несколько компаний, он обзавелся поддержкой нескольких взаимозаменяемых опций для&nbsp;практически любой выделенной части или “сервиса”:</p>
        
        <ul>
            <li>
                HDFS, Cassandra, Amazon&nbsp;S3, Google Cloud Storage или Azure Blob Storage и&nbsp;т.д. в&nbsp;качестве “глубокого хранилища”;
            </li>
            <li>
                Kafka, или RabbitMQ, Samza, или Flink, или Spark, Storm, и&nbsp;т.д. (через <a href="https://github.com/druid-io/tranquility" target="_blank">Tranquility</a>) в&nbsp;качестве источника поглощения данных в&nbsp;реальном времени;
            </li>
            <li>
                Сам Druid, или Graphite, или Ambari, или StatsD, или Kafka в качестве “слива” для&nbsp;телеметрии кластера Druid (метрик).
            </li>
        </ul>

        <p>В&nbsp;то&nbsp;же время Pinot почти целиком разрабатывался исключительно в&nbsp;стенах LinkedIn и должен был удовлетворять текущим нуждам компании, поэтому выбор, который вам предлагается, не&nbsp;так&nbsp;велик. В&nbsp;качестве “глубокого хранилища” необходимо использовать HDFS или Amazon S3, а&nbsp;для&nbsp;поглощения данных в&nbsp;реальном времени подойдет только Kafka. Но если кому-то это действительно понадобится, мне кажется, не&nbsp;составит особой сложности добавить поддержку любого другого сервиса в&nbsp;Pinot. К&nbsp;тому&nbsp;же, можно ожидать позитивных сдвигов в&nbsp;этом направлении, поскольку <a href="https://www.slideshare.net/XIANGFU3/pinot-near-realtime-analytics-uber" target="_blank">Uber</a> и Slack начинают использовать Pinot.</p>

        <h3>Формат данных и движок выполнения запросов лучше оптимизированы в&nbsp;Pinot</h3>
        <p>В&nbsp;частности, следующие функции <a href="https://github.com/linkedin/pinot/wiki/Architecture#anatomy-of-index-segment" target="_blank">формата сегментов Pinot</a> сейчас отсутствуют в&nbsp;Druid:</p>

        <ul>
            <li>
                <strong>Сжатие проиндексированных столбцов</strong> с&nbsp;битовой гранулярностью, но байтовой гранулярностью в&nbsp;Druid.
            </li>
            <li>
                <strong>Инвертированный индекс опционален</strong> для&nbsp;каждого столбца. В&nbsp;Druid он является обязательным, иногда этого не&nbsp;требуется, но все&nbsp;равно занимает много места. Различие в&nbsp;потреблении места между&nbsp;Druid и Pinot, <a href="https://www.slideshare.net/XIANGFU3/pinot-near-realtime-analytics-uber/17" target="_blank">на которое указывает Uber в&nbsp;своих тестах</a>, вполне возможно вызвано именно этим.
            </li>
            <li>
                <strong>Минимальные и максимальные значения</strong> в&nbsp;числовых столбцах записываются посегментно.
            </li>
            <li>
                <strong>Поддержка сортировки данных из&nbsp;коробки</strong>. В&nbsp;Druid этого можно достичь только вручную и слегка специфическим способом (как было описано в&nbsp;разделе “CloudFlare: ClickHouse против Druid”). Сортировка данных означает лучшее сжатие, и эта функция в&nbsp;Pinot - еще одна причина различия между Druid и Pinot в&nbsp;потреблении пространства (и производительности запросов!), на&nbsp;которую указывает Uber.
            </li>
            <li>
                <strong>Формат данных</strong>, используемый для&nbsp;многозначных столбцов, на&nbsp;данный момент лучше оптимизирован в&nbsp;Pinot, чем в&nbsp;Druid.
            </li>
        </ul>

        <p>Однако, все это можно реализовать и в&nbsp;Druid. И несмотря на&nbsp;то, что формат Pinot оптимизирован существенно лучше, чем формат Druid, он все равно достаточно далек от&nbsp;того, чтобы быть оптимальным. Один из&nbsp;примеров: Pinot (как и Druid) использует только сжатие общего назначение (как Zstd) и еще не&nbsp;реализовали идеи сжатия из&nbsp;<a href="http://www.vldb.org/pvldb/vol8/p1816-teller.pdf" target="_blank">Gorilla</a>.</p>

        <p>К&nbsp;сожалению Uber по&nbsp;большей части использовал запросы <code>count (*)</code> для&nbsp;сравнения производительности Druid и Pinot относительно выполнения запроса [<a href="https://www.slideshare.net/XIANGFU3/pinot-near-realtime-analytics-uber/18" target="_blank">1</a>,&nbsp;<a href="https://www.slideshare.net/XIANGFU3/pinot-near-realtime-analytics-uber/19" target="_blank">2</a>], который сейчас в&nbsp;Druid представляет собой тупое линейное сканирование, хотя его и несложно заменить <a href="https://github.com/druid-io/druid/issues/4065#issuecomment-286963235" target="_blank">корректной O(1) реализацией</a>. Это вам еще один пример бессмысленных сравнений в&nbsp;стиле “черного ящика”, о&nbsp;которых мы говорили ранее.</p>

        <p>По&nbsp;моему мнению, причины сильного различия в&nbsp;производительности запросов <code>GROUP BY</code>, которое наблюдали в&nbsp;Uber, стоит искать в&nbsp;недостатке сортировки данных в&nbsp;сегментах Druid, как уже было отмечено выше в&nbsp;этом разделе.</p>

        <h3>У&nbsp;Druid есть более умный алгоритм присваивания (балансировки) сегментов</h3>

        <p>Алгоритм Pinot заключается в&nbsp;присвоении сегмента к&nbsp;узлам обработки запроса, которые имеют наименьшее число сегментов, загруженных в&nbsp;текущий момент. Алгоритм Druid является гораздо более сложным; он учитывает таблицу каждого сегмента и время, и применяет <strong>сложную формулу для&nbsp;вычисления финального коэффициента</strong>, согласно которому будут ранжированы узлы обработки запросов для&nbsp;выбора наилучшего, которому и будет присвоен новый сегмент. Этот алгоритм показал ускорение в&nbsp;скорости выполнения запросов в&nbsp;продакшне Metamarkets на&nbsp;<strong>30-40%</strong>. Хотя, даже несмотря на&nbsp;подобный результат, мы им по-прежнему не&nbsp;слишком довольны - подробности можно прочитать <a href="https://metamarkets.com/2016/distributing-data-in-druid-at-petabyte-scale/" target="_blank">в&nbsp;отдельной статье.</a></p>

        <p>Не&nbsp;знаю, как в&nbsp;LinkedIn управляются со&nbsp;всем при помощи настолько простого алгоритма балансировки сегментов в&nbsp;Pinot, но, вполне возможно, их ожидают значительные улучшения по&nbsp;части производительности, если они решатся потратить время на&nbsp;совершенствование используемого ими алгоритма.</p>

        <h3>Pinot более устойчив к&nbsp;отказам при&nbsp;выполнении сложных запросов</h3>
        <p>Как уже упоминалось выше в&nbsp;разделе “Выполнение запроса”, когда брокер-узел создает подзапросы к&nbsp;другим узлам, некоторые подзапросы заканчиваются ошибкой, но Pinot объединяет результаты всех удачно выполненных подзапросов и по-прежнему возвращает частичный результат пользователю.</p>

        <p>В&nbsp;Druid такой функции на&nbsp;данный момент.</p>

        <h3>Иерархия узлов обработки запросов в&nbsp;Druid</h3>
        <p>Смотрите аналогичный раздел выше. Druid позволяет вводить уровни узлов обработки запросов для&nbsp;старых и новых данных, и для&nbsp;узлов со&nbsp;“старыми” данными соотношение “ресурсы CPU,&nbsp;RAM&nbsp;/&nbsp;число загруженных сегментов” гораздо ниже, что позволяет выиграть на&nbsp;расходах на&nbsp;инфраструктуру в&nbsp;обмен на&nbsp;низкую производительность запросов при&nbsp;доступе к&nbsp;старым данным.</p>

        <p>Насколько мне известно, в&nbsp;Pinot на&nbsp;данный момент аналогичная функциональность отсутствует.</p>

        <h2>Заключение</h2>
        <p><strong>ClickHouse, Druid и Pinot имеют фундаментально схожую архитектуру</strong>, и занимают свою собственную нишу между Big&nbsp;Data-фреймворками общего назначения вроде Impala, Presto, Spark, и колоночными базами данных с корректной поддержкой первичных ключей, точечных обновлений и удалений, как InfluxDB.</p>

        <p>В&nbsp;силу схожести архитектур, ClickHouse, Druid и Pinot имеют примерно одинаковый “предел оптимизации”. Но в&nbsp;своем текущем состоянии, <strong>все три системы еще незрелы</strong> и очень далеки от&nbsp;этого лимита. Существенных улучшений в&nbsp;производительности данных систем (применительно к&nbsp;специфическим сценариям использования) можно достичь несколькими человеко-месяцами работы опытных инженеров.</p>

        <blockquote>
            <p>Я бы не&nbsp;рекомендовал вам сравнивать производительность данных систем между&nbsp;собой - выберите для&nbsp;себя ту, чей исходный код вы способны понять и модифицировать, или ту, в&nbsp;которую вы хотите инвестировать свои ресурсы.</p>
        </blockquote>

        <p>Из&nbsp;этих трех систем, ClickHouse стоит немного в&nbsp;стороне от&nbsp;Druid и Pinot - в&nbsp;то&nbsp;время как Druid и Pinot практически идентичны, и их можно считать двумя независимо разрабатываемыми реализациями одной и&nbsp;той&nbsp;же системы.</p>

        <p>ClickHouse больше напоминает “традиционные” базы данных вроде PostgreSQL. ClickHouse можно установить на&nbsp;один узел. При&nbsp;малых масштабах (менее 1&nbsp;TB памяти, менее 100&nbsp;ядер&nbsp;CPU), ClickHouse выглядит гораздо более интересным вариантом, чем&nbsp;Druid или Pinot - если вам все еще хочется их сравнивать - в&nbsp;силу&nbsp;того, что ClickHouse проще и имеет меньше движущихся частей и сервисов. Я&nbsp;бы&nbsp;даже сказал, что на&nbsp;таком масштабе он скорее становится конкурентом для&nbsp;InfluxDB или Prometheus, а не&nbsp;для Druid или Pinot.</p>

        <p>Druid и Pinot больше напоминают другие системы Big&nbsp;Data из&nbsp;экосистемы Hadoop. Они сохраняют свои “самоуправляемые” свойства даже на&nbsp;очень больших масштабах (более 500 узлов), в&nbsp;то&nbsp;время как ClickHouse потребует для&nbsp;этого достаточно много работы профессиональных SRE. Кроме&nbsp;того, Druid и Pinot занимают выигрышную позицию в&nbsp;плане оптимизации инфраструктурной стоимости больших кластеров, и лучше подходят для&nbsp;облачных окружений, чем ClickHouse.</p>

        <p>Единственным долгосрочным различием между Druid и Pinot является то, что Pinot зависит от&nbsp;фреймворка Helix и будет продолжать зависеть от&nbsp;ZooKeeper, в&nbsp;то&nbsp;время как Druid может уйти от&nbsp;зависимости от&nbsp;ZooKeeper. С&nbsp;другой стороны, установка Druid продолжит зависеть от&nbsp;наличия какой-либо SQL-базы данных. На&nbsp;данный момент, Pinot оптимизирован лучше, чем Druid.</p>

        <blockquote>
            <p>Если вы уже сталкивались с необходимостью сравнения этих систем и сделали свой выбор, то приходите на одну из наших конференций и расскажите о своем кейсе: о том какие именно были задачи и какие грабли (а наверняка они были) вы встретили. Хотя, конечно, базы данных далеко не единственная тема. Ближайший по окончанию срока подачи заявок (<strong>до 9 апреля</strong>) фестиваль <a href="http://ritfest.ru/" target="_blank">РИТ++</a> включает направления: <a href="http://frontendconf.ru/" target="_blank">фронтенд</a>, <a href="http://backendconf.ru/" target="_blank">бэкенд</a>, <a href="http://rootconf.ru/" target="_blank">эксплуатацию</a> и <a href="http://whalerider.ru/" target="_blank">управление</a>. Участникам обычно интереснее всего узнать о конкретных примерах, но и выступления и в виде обзоров и исследований тоже возможны – главное, чтобы тема была интересна лично вам.</p>    
        </blockquote>

    </article>
</body>
</html>

